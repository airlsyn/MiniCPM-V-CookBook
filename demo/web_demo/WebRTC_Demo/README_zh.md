# WebRTC å®æ—¶è§†é¢‘äº¤äº’æ¼”ç¤º

åŸºäº WebRTC å®ç°çš„å…¨åŒå·¥å®æ—¶è§†é¢‘äº¤äº’æ–¹æ¡ˆï¼Œæ”¯æŒæµå¼è¾“å…¥è¾“å‡ºï¼Œå…·æœ‰é«˜å“åº”ã€ä½å»¶è¿Ÿçš„ç‰¹æ€§ã€‚

ğŸ“– [English Version](./README.md)

## æ¦‚è¿°

æœ¬æ¼”ç¤ºé‡‡ç”¨ WebRTC æŠ€æœ¯å®ç°äº†**å…¨åŒå·¥å®æ—¶è§†é¢‘äº¤äº’**æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆå¡«è¡¥äº†ç›®å‰å¼€æºç¤¾åŒºä¸­**æµå¼åŒå·¥å¯¹è¯æ–¹æ¡ˆ**çš„æŠ€æœ¯ç©ºç™½ï¼Œä¸ºå®æ—¶å¤šæ¨¡æ€äº¤äº’æä¾›äº†å®Œæ•´çš„è§£å†³æ–¹æ¡ˆã€‚

> [!IMPORTANT]
> **GGUF æ¨¡å‹æ›´æ–°æé†’**ï¼šGGUF æ¨¡å‹æ–‡ä»¶è¿‘æœŸæœ‰æ›´æ–°ï¼ˆåŒ…æ‹¬ `prompt_cache.gguf` ç­‰ç»„ä»¶ï¼‰ã€‚å¦‚æœæ‚¨ä¹‹å‰å·²ä¸‹è½½è¿‡æ¨¡å‹ï¼Œè¯·é‡æ–°ä¸‹è½½æœ€æ–°ç‰ˆæœ¬ä»¥ç¡®ä¿å…¼å®¹æ€§ã€‚ä½¿ç”¨æ—§ç‰ˆæ¨¡å‹æ–‡ä»¶å¯èƒ½å¯¼è‡´åˆå§‹åŒ–å¤±è´¥æˆ–éŸ³é¢‘è´¨é‡ä¸‹é™ã€‚

## ç¡¬ä»¶é…ç½®è¦æ±‚

å…¨é‡æ¨¡å‹ï¼ˆLLM Q4_K_M + Vision/Audio/TTS F16 + Token2Wavï¼‰æ€»è®¡çº¦ **8.3 GB**ï¼Œè¿è¡Œæ—¶ GPU æ˜¾å­˜å ç”¨çº¦ **10 GB**ï¼ˆå« KV cache å’Œè®¡ç®—ç¼“å†²åŒºï¼‰ã€‚

<details>
<summary><b>macOS (Apple Silicon)</b></summary>

| æ¨¡å¼ | æœ€ä½é…ç½® | æ¨èé…ç½® | è¯´æ˜ |
|------|---------|---------|------|
| **å•å·¥** | M1/M2/M3/M4, 16GB å†…å­˜ | M4 ç³»åˆ—, 32GB+ å†…å­˜ | æ‰€æœ‰ Apple Silicon èŠ¯ç‰‡å‡å¯è¿è¡Œï¼›èŠ¯ç‰‡è¶Šæ–°æ¨ç†è¶Šå¿« |
| **åŒå·¥** | M4 Pro, 36GB+ å†…å­˜ | **M4 Max, 64GB+ å†…å­˜** | å®æ—¶æµå¼äº¤äº’å¯¹å†…å­˜å¸¦å®½è¦æ±‚é«˜ï¼›M4 Max å®æµ‹ RTF < 1.0 |

> **è¯´æ˜**ï¼šmacOS ä½¿ç”¨ç»Ÿä¸€å†…å­˜æ¶æ„â€”â€”æ¨¡å‹æƒé‡ã€KV cacheã€è®¡ç®—ç¼“å†²åŒºå…±äº«ç³»ç»Ÿå†…å­˜ã€‚åŒå·¥æ¨¡å¼çš„ä¸»è¦ç“¶é¢ˆæ˜¯è®¡ç®—ååé‡è€Œéå†…å­˜å®¹é‡ã€‚M1/M2/M3 èŠ¯ç‰‡çš„å¸¦å®½å’Œç®—åŠ›ä¸è¶³ä»¥æ”¯æ’‘å®æ—¶åŒå·¥æµå¼äº¤äº’ã€‚
>
> **æç¤º**ï¼šå¦‚æœä½ çš„ Mac åªæœ‰ 16GB å†…å­˜ï¼Œè¿è¡Œå‰è¯·å…³é—­å…¶ä»–å ç”¨å†…å­˜è¾ƒå¤§çš„åº”ç”¨ï¼ˆæµè§ˆå™¨ã€IDE ç­‰ï¼‰ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿå†…å­˜ç”¨äºæ¨¡å‹åŠ è½½ã€‚

</details>

<details>
<summary><b>Linux / Windows (NVIDIA GPU)</b></summary>

| æ¨¡å¼ | æœ€ä½æ˜¾å­˜ | æ¨èæ˜¾å­˜ | ç¤ºä¾‹æ˜¾å¡ |
|------|---------|---------|---------|
| **å•å·¥** | 10 GB | 12 GB+ | RTX 3060 12GB, RTX 4070 12GB |
| **åŒå·¥** | 12 GB | 16 GB+ | RTX 4080 16GB, RTX 4090 24GB, RTX 3090 24GB |

**æ˜¾å¡é€‚é…å‚è€ƒ**ï¼š

| æ˜¾å¡ | æ˜¾å­˜ | å•å·¥ | åŒå·¥ | è¯´æ˜ |
|------|------|------|------|------|
| RTX 4060 | 8 GB | âŒ | âŒ | æ˜¾å­˜ä¸è¶³ |
| RTX 3060 | 12 GB | âœ… | âš ï¸ å‹‰å¼º | éƒ¨åˆ†æ¨¡å—å¯èƒ½éœ€è¦ CPU offload |
| RTX 4070 | 12 GB | âœ… | âœ… | åŒå·¥å…¥é—¨çº§ |
| RTX 4080 | 16 GB | âœ… | âœ… | æ¨èåŒå·¥é…ç½® |
| RTX 3090 | 24 GB | âœ… | âœ… | å……è£• |
| RTX 4090 | 24 GB | âœ… | âœ… | æœ€ä½³æ€§èƒ½ |

> **è¯´æ˜**ï¼šNVIDIA CUDA GPU åœ¨æ­¤ç±»è´Ÿè½½ä¸‹é€šå¸¸æ¯” Apple Silicon Metal æ›´å¿«ã€‚RTX 4070 å³å¯æµç•…è¿è¡ŒåŒå·¥å®æ—¶äº¤äº’ã€‚

</details>

## å¿«é€Ÿå¼€å§‹

æä¾›ä¸‰ç§éƒ¨ç½²æ–¹å¼ï¼Œä»»é€‰å…¶ä¸€ï¼š

| æ–¹å¼ | è¯´æ˜ | æ¨èåœºæ™¯ |
|------|------|---------|
| **âš¡ æ–¹æ¡ˆ Aï¼šoneclick.sh å…¨è‡ªåŠ¨éƒ¨ç½²** | ä¸€ä¸ªè„šæœ¬å…¨æå®šï¼Œè‡ªåŠ¨ä¸‹è½½æºç /æ¨¡å‹/å·¥å…·ã€ç¼–è¯‘ã€å¯åŠ¨ | **æœ€çœå¿ƒ**ï¼Œé€‚åˆæ–°æœåŠ¡å™¨ï¼Œæ— éœ€ Dockerï¼Œæ— éœ€æ‰‹åŠ¨å‰ç½®å‡†å¤‡ |
| **ğŸ³ æ–¹æ¡ˆ Bï¼šDocker éƒ¨ç½²** | ä½¿ç”¨ Docker è¿è¡Œå‰ç«¯/åç«¯ï¼Œæœ¬åœ°è¿è¡Œ C++ æ¨ç† | çµæ´»å¯æ§ï¼Œæ”¯æŒé¢„æ„å»ºé•œåƒ |

---

### âš¡ æ–¹æ¡ˆ Aï¼šoneclick.sh å…¨è‡ªåŠ¨éƒ¨ç½²ï¼ˆæ¨èï¼‰

**é›¶å‰ç½®æ¡ä»¶** â€” åªéœ€æä¾› Python è·¯å¾„ï¼Œè„šæœ¬è‡ªåŠ¨å®Œæˆæ‰€æœ‰äº‹æƒ…ï¼šä¸‹è½½æºç ã€ä¸‹è½½æ¨¡å‹ã€ç¼–è¯‘ C++ã€å®‰è£…ä¾èµ–ã€å¯åŠ¨å…¨éƒ¨æœåŠ¡ã€‚**ä¸éœ€è¦ Docker**ã€‚

```bash
# é¦–æ¬¡è¿è¡Œ â€” å…¨è‡ªåŠ¨ä¸‹è½½ã€ç¼–è¯‘ã€å¯åŠ¨ï¼ˆåŒå·¥æ¨¡å¼ï¼‰
PYTHON_CMD=/path/to/python bash oneclick.sh start

# å•å·¥æ¨¡å¼
PYTHON_CMD=/path/to/python CPP_MODE=simplex bash oneclick.sh start

# macOS ä½¿ç”¨ Metal GPUï¼ˆéƒ¨åˆ†èŠ¯ç‰‡ä¸Šæ¯” ANE æ›´å¿«ï¼‰
PYTHON_CMD=/path/to/python VISION_BACKEND=metal bash oneclick.sh start

# æŸ¥çœ‹çŠ¶æ€ / æŸ¥çœ‹æ—¥å¿— / åœæ­¢
bash oneclick.sh status
bash oneclick.sh logs
bash oneclick.sh stop
```

è„šæœ¬è‡ªåŠ¨å®Œæˆï¼š
1. âœ… ä¸‹è½½ WebRTC_Demo æºç ã€llama.cpp-omni æºç 
2. âœ… ä¸‹è½½ GGUF æ¨¡å‹ï¼ˆä» HuggingFaceï¼Œå›½å†…è‡ªåŠ¨èµ°é•œåƒï¼‰
3. âœ… å®‰è£… livekit-serverã€nodeã€pnpm ç­‰å·¥å…·
4. âœ… ç¼–è¯‘ llama-server
5. âœ… å¯åŠ¨ LiveKit â†’ Backend â†’ C++ æ¨ç† â†’ Frontendï¼ˆå…± 4 ä¸ªæœåŠ¡ï¼‰
6. âœ… è‡ªåŠ¨æ³¨å†Œæ¨ç†æœåŠ¡

å¯åŠ¨å®Œæˆåï¼Œæµè§ˆå™¨æ‰“å¼€ï¼š**https://localhost:8088**

> è¯¦ç»†ç¯å¢ƒå˜é‡å’Œè¿›é˜¶ç”¨æ³•è¯·å‚è€ƒ [oneclick.md](./oneclick.md)ã€‚

---

### ğŸ³ æ–¹æ¡ˆ Bï¼šDocker éƒ¨ç½²

ä½¿ç”¨ Docker è¿è¡Œå‰ç«¯/åç«¯/LiveKitï¼Œæœ¬åœ°è¿è¡Œ C++ æ¨ç†æœåŠ¡ã€‚

> ä»¥ä¸‹æ­¥éª¤**ä»…æ–¹æ¡ˆ B éœ€è¦**ï¼Œæ–¹æ¡ˆ A ä¼šè‡ªåŠ¨å¤„ç†ã€‚

#### 1. å®‰è£… Docker

<details>
<summary><b>macOS</b></summary>

```bash
# ä½¿ç”¨ Homebrew å®‰è£…
brew install --cask docker

# æˆ–ä»å®˜ç½‘ä¸‹è½½ï¼šhttps://www.docker.com/products/docker-desktop

# å¯åŠ¨ Docker Desktop
open -a Docker

# éªŒè¯å®‰è£…
docker --version
```

</details>

<details>
<summary><b>Linux</b></summary>

```bash
# å®‰è£… Docker Engine (Ubuntu/Debian)
sudo apt-get update
sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin

# å¯åŠ¨ Docker æœåŠ¡
sudo systemctl start docker
sudo systemctl enable docker

# ï¼ˆå¯é€‰ï¼‰å°†å½“å‰ç”¨æˆ·æ·»åŠ åˆ° docker ç»„ï¼ˆå… sudoï¼‰
sudo usermod -aG docker $USER
newgrp docker

# éªŒè¯å®‰è£…
docker --version
```

**NVIDIA GPU æ”¯æŒ**ï¼ˆGPU åŠ é€Ÿå¿…é¡»ï¼‰ï¼š

```bash
# å®‰è£… NVIDIA Container Toolkit
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

# éªŒè¯ GPU è®¿é—®
docker run --rm --gpus all nvidia/cuda:12.0-base nvidia-smi
```

</details>

<details>
<summary><b>Windows</b></summary>

1. ä¸‹è½½å¹¶å®‰è£… [Docker Desktop for Windows](https://www.docker.com/products/docker-desktop)
2. ç¡®ä¿å¯ç”¨ **WSL 2** åç«¯ï¼ˆDocker Desktop è®¾ç½® â†’ General â†’ Use the WSL 2 based engineï¼‰
3. å®‰è£…å®Œæˆåé‡å¯ç”µè„‘

```powershell
# éªŒè¯å®‰è£…ï¼ˆPowerShellï¼‰
docker --version
```

</details>

#### 2. ç¼–è¯‘ llamacpp-omni æ¨ç†æœåŠ¡

<details>
<summary><b>macOS (Apple Silicon)</b></summary>

```bash
# å…‹éš†å¹¶è¿›å…¥é¡¹ç›®ç›®å½•
git clone https://github.com/tc-mb/llama.cpp-omni.git
cd llama.cpp-omni

# ç¼–è¯‘ï¼ˆmacOS é»˜è®¤å¯ç”¨ Metal åŠ é€Ÿï¼‰
cmake -B build -DCMAKE_BUILD_TYPE=Release
cmake --build build --target llama-server -j

# éªŒè¯ç¼–è¯‘ç»“æœ
ls -la build/bin/llama-server
```

**å¯é€‰ï¼šApple Neural Engine (ANE/NPU) åŠ é€Ÿè§†è§‰ç¼–ç å™¨**

macOS æ”¯æŒé€šè¿‡ CoreML å°†è§†è§‰ç¼–ç å™¨è¿è¡Œåœ¨ Apple Neural Engineï¼ˆANE/NPUï¼‰ä¸Šï¼Œä»è€Œå°† ViT è®¡ç®—ä» GPU å¸è½½ï¼Œä¸º LLM å’Œ TTS æ¨¡å‹ç•™å‡ºæ›´å¤š GPU å¸¦å®½ã€‚å¯ç”¨æ–¹æ³•ï¼š

1. ä¸‹è½½ CoreML è§†è§‰æ¨¡å‹ï¼ˆ`coreml_minicpmo45_vit_all_f16.mlmodelc`ï¼‰ï¼Œæ”¾ç½®åœ¨ `<MODEL_DIR>/vision/` ç›®å½•ä¸‹
2. éƒ¨ç½²æ—¶æ·»åŠ  `--vision-backend coreml` å‚æ•°ï¼ˆè§ä¸‹æ–¹éƒ¨ç½²æ­¥éª¤ï¼‰

> **è¯´æ˜**ï¼šåœ¨éƒ¨åˆ†èŠ¯ç‰‡ä¸Šï¼ŒMetal GPU çš„è§†è§‰ç¼–ç é€Ÿåº¦å¯èƒ½å¿«äº ANEã€‚å»ºè®®åœ¨ä½ çš„ç¡¬ä»¶ä¸Šåˆ†åˆ«æµ‹è¯•ä¸¤ç§åç«¯ï¼Œé€‰æ‹©æ›´å¿«çš„ã€‚é»˜è®¤ä½¿ç”¨ Metal (GPU)ã€‚

</details>

<details>
<summary><b>Linux (NVIDIA GPU)</b></summary>

```bash
# å…‹éš†å¹¶è¿›å…¥é¡¹ç›®ç›®å½•
git clone https://github.com/tc-mb/llama.cpp-omni.git
cd llama.cpp-omni

# ç¼–è¯‘ï¼ˆå¯ç”¨ CUDA åŠ é€Ÿï¼‰
cmake -B build -DCMAKE_BUILD_TYPE=Release -DGGML_CUDA=ON
cmake --build build --target llama-server -j

# éªŒè¯ç¼–è¯‘ç»“æœ
ls -la build/bin/llama-server
```

> **æ³¨æ„**ï¼šéœ€è¦é¢„å…ˆå®‰è£… NVIDIA é©±åŠ¨å’Œ CUDA Toolkitï¼Œå¯é€šè¿‡ `nvidia-smi` éªŒè¯ã€‚

</details>

<details>
<summary><b>Windows (NVIDIA GPU)</b></summary>

**å‰æ**ï¼šå·²å®‰è£… Visual Studio 2019+ï¼ˆå« C++ å·¥ä½œè´Ÿè½½ï¼‰ã€CMakeã€CUDA Toolkitã€‚

```powershell
# å…‹éš†å¹¶è¿›å…¥é¡¹ç›®ç›®å½•
git clone https://github.com/tc-mb/llama.cpp-omni.git
cd llama.cpp-omni

# ç¼–è¯‘ï¼ˆå¯ç”¨ CUDA åŠ é€Ÿï¼‰
cmake -B build -DCMAKE_BUILD_TYPE=Release -DGGML_CUDA=ON
cmake --build build --config Release --target llama-server -j

# éªŒè¯ç¼–è¯‘ç»“æœ
dir build\bin\Release\llama-server.exe
```

> **æ³¨æ„**ï¼šå¦‚æœæ²¡æœ‰ NVIDIA GPUï¼Œå»æ‰ `-DGGML_CUDA=ON` ä»¥ä½¿ç”¨çº¯ CPU æ¨¡å¼ã€‚

</details>

#### 3. å‡†å¤‡ GGUF æ¨¡å‹æ–‡ä»¶

æˆ‘ä»¬æä¾›äº†**ä¸€é”®ä¸‹è½½è„šæœ¬** `download_models.sh`ï¼Œè‡ªåŠ¨ä¸‹è½½æ‰€æœ‰æ‰€éœ€çš„æ¨¡å‹æ–‡ä»¶ï¼ˆå…±çº¦ 8.3GBï¼‰ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ ã€‚

<details>
<summary><b>ä¸‹è½½å‘½ä»¤å’Œæ¨¡å‹æ–‡ä»¶è¯´æ˜</b></summary>

```bash
# ä¸‹è½½æ‰€æœ‰å¿…éœ€çš„ GGUF æ¨¡å‹ï¼ˆè‡ªåŠ¨é€‰æ‹©æœ€å¿«æºï¼‰
./download_models.sh --model-dir /path/to/gguf

# ä½¿ç”¨ ModelScopeï¼ˆå›½å†…æ›´å¿«ï¼‰
./download_models.sh --model-dir /path/to/gguf --source ms

# ä½¿ç”¨ HuggingFace é•œåƒ
./download_models.sh --model-dir /path/to/gguf --hf-mirror https://hf-mirror.com

# é€‰æ‹©ä¸åŒçš„ LLM é‡åŒ–ç‰ˆæœ¬ï¼ˆé»˜è®¤ Q4_K_Mï¼‰
./download_models.sh --model-dir /path/to/gguf --quant Q8_0
```

è„šæœ¬ä¸‹è½½ä»¥ä¸‹æ–‡ä»¶ï¼š

```
<MODEL_DIR>/
â”œâ”€â”€ MiniCPM-o-4_5-Q4_K_M.gguf        # LLM ä¸»æ¨¡å‹ (~5GB)
â”œâ”€â”€ audio/                            # éŸ³é¢‘ç¼–ç å™¨
â”‚   â””â”€â”€ MiniCPM-o-4_5-audio-F16.gguf
â”œâ”€â”€ vision/                           # è§†è§‰ç¼–ç å™¨
â”‚   â””â”€â”€ MiniCPM-o-4_5-vision-F16.gguf
â”œâ”€â”€ tts/                              # TTS æ¨¡å‹
â”‚   â”œâ”€â”€ MiniCPM-o-4_5-tts-F16.gguf
â”‚   â””â”€â”€ MiniCPM-o-4_5-projector-F16.gguf
â””â”€â”€ token2wav-gguf/                   # Token2Wav æ¨¡å‹
    â”œâ”€â”€ encoder.gguf
    â”œâ”€â”€ flow_matching.gguf
    â”œâ”€â”€ flow_extra.gguf
    â”œâ”€â”€ hifigan2.gguf
    â””â”€â”€ prompt_cache.gguf
```

å¯é€‰ LLM é‡åŒ–ç‰ˆæœ¬ï¼š`Q4_0`ã€`Q4_1`ã€`Q4_K_M`ï¼ˆæ¨èï¼‰ã€`Q4_K_S`ã€`Q5_0`ã€`Q5_1`ã€`Q5_K_M`ã€`Q5_K_S`ã€`Q6_K`ã€`Q8_0`ã€`F16`

</details>

#### 4. å¯é€‰ï¼šåŠ è½½é¢„æ„å»º Docker é•œåƒ

å¦‚æœä¸æƒ³è‡ªè¡Œæ„å»ºå‰ç«¯/åç«¯é•œåƒï¼Œå¯ä¸‹è½½é¢„æ„å»ºé•œåƒï¼š

ğŸ“¦ [ä¸‹è½½ Docker é•œåƒ](https://drive.google.com/file/d/191h2OJYir9aAL4KIE-mFF_XJ1jT6gnxj/view?usp=sharing)

```bash
# è§£å‹å¹¶åŠ è½½é•œåƒï¼ˆå·²æœ‰é•œåƒå¯è·³è¿‡æ­¤æ­¥ï¼‰
docker load -i o45-frontend.tar
docker load -i omini_backend_code/omni_backend.tar
```

#### 5. ä¸€é”®å¯åŠ¨

<details>
<summary><b>macOS / Linux (deploy_all.sh)</b></summary>

```bash
cd WebRTC_Demo

# å•å·¥æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
./deploy_all.sh \
    --cpp-dir /path/to/llama.cpp-omni \
    --model-dir /path/to/gguf

# åŒå·¥æ¨¡å¼
./deploy_all.sh \
    --cpp-dir /path/to/llama.cpp-omni \
    --model-dir /path/to/gguf \
    --duplex
```

**macOS ä¸“å±é€‰é¡¹**ï¼š

```bash
# ä½¿ç”¨ Apple Neural Engine (ANE/NPU) åŠ é€Ÿè§†è§‰ç¼–ç å™¨
./deploy_all.sh \
    --cpp-dir /path/to/llama.cpp-omni \
    --model-dir /path/to/gguf \
    --duplex \
    --vision-backend coreml

# æ‰‹åŠ¨æŒ‡å®š Python è·¯å¾„
./deploy_all.sh \
    --cpp-dir /path/to/llama.cpp-omni \
    --model-dir /path/to/gguf \
    --python /path/to/python3
```

> **æç¤º**ï¼š`--vision-backend coreml` å°†è§†è§‰ç¼–ç å™¨è¿è¡Œåœ¨ NPU ä¸Šï¼Œé‡Šæ”¾ GPU ç»™ LLM/TTS ä½¿ç”¨ã€‚é»˜è®¤å€¼ä¸º `metal`ï¼ˆGPUï¼‰ã€‚å»ºè®®åœ¨ä½ çš„ç¡¬ä»¶ä¸Šåˆ†åˆ«æµ‹è¯•ä¸¤ç§æ–¹æ¡ˆï¼Œé€‰æ‹©å»¶è¿Ÿæ›´ä½çš„ã€‚

</details>

<details>
<summary><b>Windows (deploy_all_win.ps1)</b></summary>

```powershell
cd WebRTC_Demo
Set-ExecutionPolicy -ExecutionPolicy Bypass -Scope Process

# å•å·¥æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
.\deploy_all_win.ps1 `
    -CppDir "C:\path\to\llama.cpp-omni" `
    -ModelDir "C:\path\to\gguf"

# åŒå·¥æ¨¡å¼
.\deploy_all_win.ps1 `
    -CppDir "C:\path\to\llama.cpp-omni" `
    -ModelDir "C:\path\to\gguf" `
    -Mode duplex
```

</details>

è„šæœ¬è‡ªåŠ¨å®Œæˆï¼šå¯åŠ¨ Docker æœåŠ¡ï¼ˆå‰ç«¯ã€åç«¯ã€LiveKitï¼‰â†’ å®‰è£… Python ä¾èµ– â†’ å¯åŠ¨ C++ æ¨ç† â†’ æ³¨å†ŒæœåŠ¡

å¯åŠ¨å®Œæˆåï¼Œæµè§ˆå™¨æ‰“å¼€ï¼š**http://localhost:3000**

> æ‰‹åŠ¨é€æ­¥éƒ¨ç½²è¯·å‚è€ƒ [DEPLOY.md](./DEPLOY.md)ã€‚

### æœåŠ¡ç«¯å£è¯´æ˜

| æœåŠ¡ | ç«¯å£ | è¯´æ˜ |
|------|------|------|
| å‰ç«¯ | 3000 | Web UI |
| åç«¯ | 8025 | åç«¯ API |
| LiveKit | 7880 | å®æ—¶é€šä¿¡ |
| æ¨ç†æœåŠ¡ | 9060 | C++ HTTP API |

### å¸¸è§é—®é¢˜

<details>
<summary><b>macOSï¼šç«¯å£ 8021 è¢«ç³»ç»ŸæœåŠ¡å ç”¨</b></summary>

macOS ç³»ç»ŸæœåŠ¡å¯èƒ½ä¼šå ç”¨ 8021 ç«¯å£ã€‚éƒ¨ç½²è„šæœ¬é»˜è®¤ä½¿ç”¨ 8025 ç«¯å£ä»¥é¿å…å†²çªã€‚

```bash
# æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨
lsof -i :8021
```

</details>

<details>
<summary><b>Linuxï¼šDocker æƒé™ä¸è¶³</b></summary>

```bash
# å°†ç”¨æˆ·æ·»åŠ åˆ° docker ç»„
sudo usermod -aG docker $USER
newgrp docker

# æˆ–ä½¿ç”¨ sudo è¿è¡Œ
sudo ./deploy_all.sh ...
```

</details>

<details>
<summary><b>Windowsï¼šè„šæœ¬æ‰§è¡Œç­–ç•¥é”™è¯¯</b></summary>

```powershell
# å…è®¸å½“å‰ä¼šè¯æ‰§è¡Œè„šæœ¬
Set-ExecutionPolicy -ExecutionPolicy Bypass -Scope Process

# æˆ–ç›´æ¥è¿è¡Œ
powershell -ExecutionPolicy Bypass -File .\deploy_all_win.ps1
```

</details>

## æ ¸å¿ƒç‰¹æ€§

### ğŸ”„ å…¨åŒå·¥é€šä¿¡
- æ”¯æŒéŸ³è§†é¢‘åŒå‘åŒæ—¶ä¼ è¾“
- è‡ªç„¶æµç•…çš„å¯¹è¯ä½“éªŒï¼Œæ— éœ€ç­‰å¾…è½®æ¬¡åˆ‡æ¢

### âš¡ é«˜å“åº”ä½å»¶è¿Ÿ
- æµå¼è¾“å…¥è¾“å‡ºï¼Œå®ç°å®æ—¶äº¤äº’
- ç«¯åˆ°ç«¯å»¶è¿Ÿä¼˜åŒ–
- å¯¹è¯è¿‡ç¨‹ä¸­å³æ—¶åé¦ˆ

### ğŸš€ åŸç”Ÿæ”¯æŒ llamacpp-omni
- æ— ç¼é›†æˆ [llamacpp-omni](https://github.com/tc-mb/llama.cpp-omni) ä½œä¸ºæ¨ç†åç«¯
- å¿«é€Ÿéƒ¨ç½²ï¼Œç®€å•é…ç½®
- é«˜æ•ˆçš„èµ„æºåˆ©ç”¨

### ğŸ¯ å¿«é€Ÿä½“éªŒ MiniCPM-o 4.5
- å¿«é€Ÿä½“éªŒ MiniCPM-o 4.5 çš„å®Œæ•´èƒ½åŠ›
- å®æ—¶å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆ
- è¯­éŸ³ä¸è§†é¢‘äº¤äº’ä¸€ä½“åŒ–

## æŠ€æœ¯äº®ç‚¹

- **WebRTC åè®®**ï¼šä¸šç•Œæ ‡å‡†çš„å®æ—¶é€šä¿¡åè®®
- **æµå¼æ¶æ„**ï¼šè¿ç»­æ•°æ®æµï¼Œäº¤äº’æµç•…
- **åŒå·¥è®¾è®¡**ï¼šå¡«è¡¥å¼€æºç¤¾åŒºæµå¼åŒå·¥å¯¹è¯æ–¹æ¡ˆçš„ç©ºç™½
- **è·¨å¹³å°æ”¯æŒ**ï¼šæ”¯æŒ macOS (Metal)ã€Linux (CUDA)ã€Windows (CUDA)

## ç›¸å…³èµ„æº

- [MiniCPM-o 4.5 æ¨¡å‹](https://huggingface.co/openbmb/MiniCPM-o-4_5)
- [llamacpp-omni æ¨ç†åç«¯](https://github.com/tc-mb/llama.cpp-omni)
